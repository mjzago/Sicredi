{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define o número de linhas a serem puladas\n",
    "linhas_para_pular = 3  # Defina o número de linhas a serem puladas, incluindo o cabeçalho\n",
    "\n",
    "# Lê o arquivo CSV pulando as linhas iniciais\n",
    "df = pd.read_csv('202310COOPERATIVAS.CSV', delimiter=';', skiprows=linhas_para_pular, encoding='ISO-8859-1')\n",
    "\n",
    "# Exibe as primeiras linhas do DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtra as linhas onde o valor da coluna \"NOME_INSTITUICAO\" contém a string \"Sicredi\" (ignorando maiúsculas e minúsculas) e remove duplicatas\n",
    "cooperativas_sicredi = df[df['NOME_INSTITUICAO'].str.contains('Sicredi', case=False)].drop_duplicates(subset=['NOME_INSTITUICAO'])\n",
    "\n",
    "cooperativas_sicredi.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Defina o número de linhas a serem puladas, incluindo o cabeçalho\n",
    "linhas_para_pular = 3\n",
    "\n",
    "# Lê o arquivo CSV pulando as linhas iniciais\n",
    "df = pd.read_csv('202310COOPERATIVAS.CSV', delimiter=';', skiprows=linhas_para_pular, encoding='ISO-8859-1')\n",
    "\n",
    "# Filtra as linhas onde o valor da coluna \"NOME_INSTITUICAO\" contém a string \"Sicredi\" (ignorando maiúsculas e minúsculas) e remove duplicatas\n",
    "cooperativas_sicredi = df[df['NOME_INSTITUICAO'].str.contains('Sicredi', case=False)].drop_duplicates(subset=['NOME_INSTITUICAO'])\n",
    "\n",
    "# Configurações do perfil do Chrome para aceitar cookies automaticamente\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--enable-automation\")\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "chrome_options.add_argument(\"--disable-extensions\")\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")\n",
    "\n",
    "# Habilitar o gerenciamento de cookies no perfil\n",
    "chrome_options.add_argument(\"--enable-blink-features=AutomaticCookieManagementEnabled\")\n",
    "\n",
    "# Caminho para o perfil do Chrome (você pode alterar este caminho conforme necessário)\n",
    "chrome_options.add_argument(\"--user-data-dir=/path/to/your/chrome/profile\")\n",
    "\n",
    "# Inicializar o driver do Chrome com as configurações do perfil\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "try:\n",
    "    # Abrir a página de busca\n",
    "    driver.get(\"https://www.sicredi.com.br/site/busca-resultado/\")\n",
    "\n",
    "    # Iterar sobre as informações da coluna NOME_INSTITUICAO de cooperativas_sicredi\n",
    "    for index, row in cooperativas_sicredi.iterrows():\n",
    "        # Obter o valor da coluna \"NOME_INSTITUICAO\" atual\n",
    "        search_query = row['NOME_INSTITUICAO']\n",
    "\n",
    "        # Esperar até que o campo de busca esteja disponível\n",
    "        search_box = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \".blocoFormulario input.inputGeral\"))\n",
    "        )\n",
    "\n",
    "        # Limpar o campo de busca\n",
    "        search_box.clear()\n",
    "\n",
    "        # Preencher o campo de busca com o valor atual da coluna\n",
    "        search_box.send_keys(search_query)\n",
    "        search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "        # Esperar até que o contêiner de resultados da busca esteja presente\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"gs-webResult.gs-result\"))\n",
    "        )\n",
    "\n",
    "        # Obter o HTML da página após a busca\n",
    "        page_source = driver.page_source\n",
    "\n",
    "        # Parse HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # Find the first search result element\n",
    "        first_search_result = soup.find(\"div\", class_=\"gs-webResult gs-result\")\n",
    "\n",
    "        # Extract the URL from the first search result element\n",
    "        if first_search_result:\n",
    "            url_element = first_search_result.find(\"a\", class_=\"gs-title\")\n",
    "            if url_element:\n",
    "                result_url = url_element.get(\"href\")\n",
    "                print(f\"URL para '{search_query}': {result_url}\")\n",
    "\n",
    "finally:\n",
    "    # Fechar o navegador\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Defina o número de linhas a serem puladas, incluindo o cabeçalho\n",
    "linhas_para_pular = 3\n",
    "\n",
    "# Lê o arquivo CSV pulando as linhas iniciais\n",
    "df = pd.read_csv('202310COOPERATIVAS.CSV', delimiter=';', skiprows=linhas_para_pular, encoding='ISO-8859-1')\n",
    "\n",
    "# Filtra as linhas onde o valor da coluna \"NOME_INSTITUICAO\" contém a string \"Sicredi\" (ignorando maiúsculas e minúsculas) e remove duplicatas\n",
    "cooperativas_sicredi = df[df['NOME_INSTITUICAO'].str.contains('Sicredi', case=False)].drop_duplicates(subset=['NOME_INSTITUICAO'])\n",
    "\n",
    "# Configurações do perfil do Chrome para aceitar cookies automaticamente\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--enable-automation\")\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "chrome_options.add_argument(\"--disable-extensions\")\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")\n",
    "\n",
    "# Habilitar o gerenciamento de cookies no perfil\n",
    "chrome_options.add_argument(\"--enable-blink-features=AutomaticCookieManagementEnabled\")\n",
    "\n",
    "# Caminho para o perfil do Chrome (você pode alterar este caminho conforme necessário)\n",
    "chrome_options.add_argument(\"--user-data-dir=/path/to/your/chrome/profile\")\n",
    "\n",
    "# Inicializar o driver do Chrome com as configurações do perfil\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "import time  # Importe o módulo time\n",
    "\n",
    "# Defina o número de segundos de atraso entre cada busca\n",
    "atraso_entre_buscas = 20  # Altere conforme necessário\n",
    "\n",
    "try:\n",
    "    # Abrir a página de busca\n",
    "    driver.get(\"https://www.sicredi.com.br/site/busca-resultado/\")\n",
    "\n",
    "    # Iterar sobre as informações da coluna NOME_INSTITUICAO de cooperativas_sicredi\n",
    "    for index, row in cooperativas_sicredi.iterrows():\n",
    "        # Obter o valor da coluna \"NOME_INSTITUICAO\" atual\n",
    "        search_query = row['NOME_INSTITUICAO']\n",
    "\n",
    "        # Esperar até que o campo de busca esteja disponível\n",
    "        search_box = WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \".blocoFormulario input.inputGeral\"))\n",
    "        )\n",
    "\n",
    "        # Limpar o campo de busca\n",
    "        search_box.clear()\n",
    "\n",
    "        # Preencher o campo de busca com o valor atual da coluna\n",
    "        search_box.send_keys(search_query)\n",
    "        search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "        # Esperar até que o contêiner de resultados da busca esteja presente\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"gs-webResult.gs-result\"))\n",
    "        )\n",
    "\n",
    "        # Obter o HTML da página após a busca\n",
    "        page_source = driver.page_source\n",
    "\n",
    "        # Parse HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # Find the first search result element\n",
    "        first_search_result = soup.find(\"div\", class_=\"gs-webResult gs-result\")\n",
    "\n",
    "        # Extract the URL from the first search result element\n",
    "        if first_search_result:\n",
    "            url_element = first_search_result.find(\"a\", class_=\"gs-title\")\n",
    "            if url_element:\n",
    "                result_url = url_element.get(\"href\")\n",
    "                print(f\"URL para '{search_query}': {result_url}\")\n",
    "\n",
    "        # Adicionar um atraso antes da próxima busca\n",
    "        time.sleep(atraso_entre_buscas)\n",
    "\n",
    "finally:\n",
    "    # Fechar o navegador\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Defina o número de linhas a serem puladas, incluindo o cabeçalho\n",
    "linhas_para_pular = 3\n",
    "\n",
    "# Lê o arquivo CSV pulando as linhas iniciais\n",
    "df = pd.read_csv('202310COOPERATIVAS.CSV', delimiter=';', skiprows=linhas_para_pular, encoding='ISO-8859-1')\n",
    "\n",
    "# Filtra as linhas onde o valor da coluna \"NOME_INSTITUICAO\" contém a string \"Sicredi\" (ignorando maiúsculas e minúsculas) e remove duplicatas\n",
    "cooperativas_sicredi = df[df['NOME_INSTITUICAO'].str.contains('Sicredi', case=False)].drop_duplicates(subset=['NOME_INSTITUICAO'])\n",
    "\n",
    "# Adiciona uma nova coluna chamada \"URL\" à tabela cooperativas_sicredi\n",
    "cooperativas_sicredi['URL'] = \"\"\n",
    "\n",
    "# Configurações do perfil do Chrome para aceitar cookies automaticamente\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--enable-automation\")\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "chrome_options.add_argument(\"--disable-extensions\")\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")\n",
    "\n",
    "# Habilitar o gerenciamento de cookies no perfil\n",
    "chrome_options.add_argument(\"--enable-blink-features=AutomaticCookieManagementEnabled\")\n",
    "\n",
    "# Inicializar o driver do Chrome com as configurações do perfil\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# Defina o número de segundos de atraso entre cada busca\n",
    "atraso_entre_buscas = 15  # Altere conforme necessário\n",
    "\n",
    "try:\n",
    "    # Abrir a página de busca\n",
    "    driver.get(\"https://www.sicredi.com.br/site/busca-resultado/\")\n",
    "\n",
    "    # Iterar sobre as informações da coluna NOME_INSTITUICAO de cooperativas_sicredi\n",
    "    for index, row in cooperativas_sicredi.iterrows():\n",
    "        # Obter o valor da coluna \"NOME_INSTITUICAO\" atual\n",
    "        search_query = row['NOME_INSTITUICAO']\n",
    "\n",
    "        # Esperar até que o campo de busca esteja disponível\n",
    "        search_box = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \".blocoFormulario input.inputGeral\"))\n",
    "        )\n",
    "\n",
    "        # Limpar o campo de busca\n",
    "        search_box.clear()\n",
    "\n",
    "        # Preencher o campo de busca com o valor atual da coluna\n",
    "        search_box.send_keys(search_query)\n",
    "        search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "        # Esperar até que o contêiner de resultados da busca esteja presente\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"gs-webResult.gs-result\"))\n",
    "        )\n",
    "\n",
    "        # Obter o HTML da página após a busca\n",
    "        page_source = driver.page_source\n",
    "\n",
    "        # Parse HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # Find the first search result element\n",
    "        first_search_result = soup.find(\"div\", class_=\"gs-webResult gs-result\")\n",
    "\n",
    "        # Extract the URL from the first search result element\n",
    "        if first_search_result:\n",
    "            url_element = first_search_result.find(\"a\", class_=\"gs-title\")\n",
    "            if url_element:\n",
    "                result_url = url_element.get(\"href\")\n",
    "                cooperativas_sicredi.at[index, 'URL'] = result_url  # Atualiza a coluna 'URL' com o resultado encontrado\n",
    "            else:\n",
    "                cooperativas_sicredi.at[index, 'URL'] = \"Não encontrado\"  # Se não encontrar o URL, escreve \"Não encontrado\" na coluna 'URL'\n",
    "\n",
    "        # Adicionar um atraso antes da próxima busca\n",
    "        time.sleep(atraso_entre_buscas)\n",
    "\n",
    "finally:\n",
    "    # Fechar o navegador\n",
    "    driver.quit()\n",
    "\n",
    "# Exibe as primeiras linhas do DataFrame com a nova coluna 'URL'\n",
    "print(cooperativas_sicredi.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium_stealth import stealth\n",
    "\n",
    "# Defina o número de linhas a serem puladas, incluindo o cabeçalho\n",
    "linhas_para_pular = 3\n",
    "\n",
    "# Lê o arquivo CSV pulando as linhas iniciais\n",
    "df = pd.read_csv('202310COOPERATIVAS.CSV', delimiter=';', skiprows=linhas_para_pular, encoding='ISO-8859-1')\n",
    "\n",
    "# Filtra as linhas onde o valor da coluna \"NOME_INSTITUICAO\" contém a string \"Sicredi\" (ignorando maiúsculas e minúsculas) e remove duplicatas\n",
    "cooperativas_sicredi = df[df['NOME_INSTITUICAO'].str.contains('Sicredi', case=False)].drop_duplicates(subset=['NOME_INSTITUICAO'])\n",
    "\n",
    "# Configurações do perfil do Chrome para aceitar cookies automaticamente\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "chrome_options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "\n",
    "# Inicializar o driver do Chrome com as configurações do perfil\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# Aplicar o Selenium Stealth\n",
    "stealth(driver,\n",
    "       user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.5481.105 Safari/537.36',\n",
    "       languages=[\"en-US\", \"en\"],\n",
    "       vendor=\"Google Inc.\",\n",
    "       platform=\"Win32\",\n",
    "       webgl_vendor=\"Intel Inc.\",\n",
    "       renderer=\"Intel Iris OpenGL Engine\",\n",
    "       fix_hairline=True,\n",
    "       )\n",
    "\n",
    "# Defina o número de segundos de atraso entre cada busca\n",
    "atraso_entre_buscas = 10  # Altere conforme necessário\n",
    "\n",
    "try:\n",
    "    # Abrir a página de busca\n",
    "    driver.get(\"https://www.sicredi.com.br/site/busca-resultado/\")\n",
    "\n",
    "    # Iterar sobre as informações da coluna NOME_INSTITUICAO de cooperativas_sicredi\n",
    "    for index, row in cooperativas_sicredi.iterrows():\n",
    "        # Obter o valor da coluna \"NOME_INSTITUICAO\" atual\n",
    "        search_query = row['NOME_INSTITUICAO']\n",
    "\n",
    "        # Esperar até que o campo de busca esteja disponível\n",
    "        search_box = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \".blocoFormulario input.inputGeral\"))\n",
    "        )\n",
    "\n",
    "        # Limpar o campo de busca\n",
    "        search_box.clear()\n",
    "\n",
    "        # Preencher o campo de busca com o valor atual da coluna\n",
    "        search_box.send_keys(search_query)\n",
    "        search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "        # Esperar até que o contêiner de resultados da busca esteja presente\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"gs-webResult.gs-result\"))\n",
    "        )\n",
    "\n",
    "        # Obter o HTML da página após a busca\n",
    "        page_source = driver.page_source\n",
    "\n",
    "        # Parse HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # Find the first search result element\n",
    "        first_search_result = soup.find(\"div\", class_=\"gs-webResult gs-result\")\n",
    "\n",
    "        # Extract the URL from the first search result element\n",
    "        if first_search_result:\n",
    "            url_element = first_search_result.find(\"a\", class_=\"gs-title\")\n",
    "            if url_element:\n",
    "                result_url = url_element.get(\"href\")\n",
    "                print(f\"URL para '{search_query}': {result_url}\")\n",
    "\n",
    "        # Adicionar um atraso antes da próxima busca\n",
    "        time.sleep(atraso_entre_buscas)\n",
    "\n",
    "finally:\n",
    "    # Fechar o navegador\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium_stealth import stealth\n",
    "import logging\n",
    "\n",
    "# Desabilitar todos os logs do Selenium\n",
    "logging.disable(logging.CRITICAL)\n",
    "\n",
    "# Defina o número de linhas a serem puladas, incluindo o cabeçalho\n",
    "linhas_para_pular = 3\n",
    "\n",
    "# Lê o arquivo CSV pulando as linhas iniciais\n",
    "df = pd.read_csv('202310COOPERATIVAS.CSV', delimiter=';', skiprows=linhas_para_pular, encoding='ISO-8859-1')\n",
    "\n",
    "# Filtra as linhas onde o valor da coluna \"NOME_INSTITUICAO\" contém a string \"Sicredi\" (ignorando maiúsculas e minúsculas) e remove duplicatas\n",
    "cooperativas_sicredi = df[df['NOME_INSTITUICAO'].str.contains('Sicredi', case=False)].drop_duplicates(subset=['NOME_INSTITUICAO'])\n",
    "\n",
    "# Configurações do perfil do Chrome para aceitar cookies automaticamente\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "chrome_options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "\n",
    "# Inicializar o driver do Chrome com as configurações do perfil\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# Aplicar o Selenium Stealth\n",
    "stealth(driver,\n",
    "       user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.5481.105 Safari/537.36',\n",
    "       languages=[\"en-US\", \"en\"],\n",
    "       vendor=\"Google Inc.\",\n",
    "       platform=\"Win32\",\n",
    "       webgl_vendor=\"Intel Inc.\",\n",
    "       renderer=\"Intel Iris OpenGL Engine\",\n",
    "       fix_hairline=True,\n",
    "       )\n",
    "\n",
    "# Defina o número de segundos de atraso entre cada busca\n",
    "atraso_entre_buscas = 10  # Altere conforme necessário\n",
    "\n",
    "try:\n",
    "    # Abrir a página de busca\n",
    "    driver.get(\"https://www.sicredi.com.br/site/busca-resultado/\")\n",
    "\n",
    "    # Iterar sobre as informações da coluna NOME_INSTITUICAO de cooperativas_sicredi\n",
    "    for index, row in cooperativas_sicredi.iterrows():\n",
    "        # Obter o valor da coluna \"NOME_INSTITUICAO\" atual\n",
    "        search_query = row['NOME_INSTITUICAO']\n",
    "\n",
    "        # Esperar até que o campo de busca esteja disponível\n",
    "        search_box = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \".blocoFormulario input.inputGeral\"))\n",
    "        )\n",
    "\n",
    "        # Limpar o campo de busca\n",
    "        search_box.clear()\n",
    "\n",
    "        # Preencher o campo de busca com o valor atual da coluna\n",
    "        search_box.send_keys(search_query)\n",
    "        search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "        # Esperar até que o contêiner de resultados da busca esteja presente\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"gs-webResult.gs-result\"))\n",
    "        )\n",
    "\n",
    "        # Obter o HTML da página após a busca\n",
    "        page_source = driver.page_source\n",
    "\n",
    "        # Parse HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # Find the first search result element\n",
    "        first_search_result = soup.find(\"div\", class_=\"gs-webResult gs-result\")\n",
    "\n",
    "        # Extract the URL from the first search result element\n",
    "        if first_search_result:\n",
    "            url_element = first_search_result.find(\"a\", class_=\"gs-title\")\n",
    "            if url_element:\n",
    "                result_url = url_element.get(\"href\")\n",
    "                print(f\"URL para '{search_query}': {result_url}\")\n",
    "\n",
    "        # Adicionar um atraso antes da próxima busca\n",
    "        time.sleep(atraso_entre_buscas)\n",
    "\n",
    "finally:\n",
    "    # Fechar o navegador\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Defina o número de linhas a serem puladas, incluindo o cabeçalho\n",
    "linhas_para_pular = 3\n",
    "\n",
    "# Lê o arquivo CSV pulando as linhas iniciais\n",
    "df = pd.read_csv('202310COOPERATIVAS.CSV', delimiter=';', skiprows=linhas_para_pular, encoding='ISO-8859-1')\n",
    "\n",
    "# Filtra as linhas onde o valor da coluna \"NOME_INSTITUICAO\" contém a string \"Sicredi\" (ignorando maiúsculas e minúsculas) e remove duplicatas\n",
    "cooperativas_sicredi = df[df['NOME_INSTITUICAO'].str.contains('Sicredi', case=False)].drop_duplicates(subset=['NOME_INSTITUICAO'])\n",
    "\n",
    "# Configurações do perfil do Chrome para aceitar cookies automaticamente\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--enable-automation\")\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "chrome_options.add_argument(\"--enable-extensions\")\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")\n",
    "\n",
    "# Habilitar o gerenciamento de cookies no perfil\n",
    "chrome_options.add_argument(\"--enable-blink-features=AutomaticCookieManagementEnabled\")\n",
    "\n",
    "# Caminho para o perfil do Chrome (você pode alterar este caminho conforme necessário)\n",
    "chrome_options.add_argument(\"--user-data-dir=/path/to/your/chrome/profile\")\n",
    "\n",
    "# Inicializar o driver do Chrome com as configurações do perfil\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "import time  # Importe o módulo time\n",
    "\n",
    "# Defina o número de segundos de atraso entre cada busca\n",
    "atraso_entre_buscas = 20  # Altere conforme necessário\n",
    "\n",
    "try:\n",
    "    # Abrir a página de busca\n",
    "    driver.get(\"https://www.sicredi.com.br/site/busca-resultado/\")\n",
    "\n",
    "    # Iterar sobre as informações da coluna NOME_INSTITUICAO de cooperativas_sicredi\n",
    "    for index, row in cooperativas_sicredi.iterrows():\n",
    "        # Obter o valor da coluna \"NOME_INSTITUICAO\" atual\n",
    "        search_query = row['NOME_INSTITUICAO']\n",
    "\n",
    "        # Esperar até que o campo de busca esteja disponível\n",
    "        search_box = WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \".blocoFormulario input.inputGeral\"))\n",
    "        )\n",
    "\n",
    "        # Limpar o campo de busca\n",
    "        search_box.clear()\n",
    "\n",
    "        # Preencher o campo de busca com o valor atual da coluna\n",
    "        search_box.send_keys(search_query)\n",
    "        search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "        # Esperar até que o contêiner de resultados da busca esteja presente\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"gs-webResult.gs-result\"))\n",
    "        )\n",
    "\n",
    "        # Obter o HTML da página após a busca\n",
    "        page_source = driver.page_source\n",
    "\n",
    "        # Parse HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # Find the first search result element\n",
    "        first_search_result = soup.find(\"div\", class_=\"gs-webResult gs-result\")\n",
    "\n",
    "        # Extract the URL from the first search result element\n",
    "        if first_search_result:\n",
    "            url_element = first_search_result.find(\"a\", class_=\"gs-title\")\n",
    "            if url_element:\n",
    "                result_url = url_element.get(\"href\")\n",
    "                print(f\"URL para '{search_query}': {result_url}\")\n",
    "\n",
    "        # Adicionar um atraso antes da próxima busca\n",
    "        time.sleep(atraso_entre_buscas)\n",
    "\n",
    "finally:\n",
    "    # Fechar o navegador\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Defina o número de linhas a serem puladas, incluindo o cabeçalho\n",
    "linhas_para_pular = 3\n",
    "\n",
    "# Lê o arquivo CSV pulando as linhas iniciais\n",
    "df = pd.read_csv('202310COOPERATIVAS.CSV', delimiter=';', skiprows=linhas_para_pular, encoding='ISO-8859-1')\n",
    "\n",
    "# Filtra as linhas onde o valor da coluna \"NOME_INSTITUICAO\" contém a string \"Sicredi\" (ignorando maiúsculas e minúsculas) e remove duplicatas\n",
    "cooperativas_sicredi = df[df['NOME_INSTITUICAO'].str.contains('Sicredi', case=False)].drop_duplicates(subset=['NOME_INSTITUICAO'])\n",
    "\n",
    "# Configurações do perfil do Chrome para aceitar cookies automaticamente e remover a flag navigator.webdriver\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--enable-automation\")\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "chrome_options.add_argument(\"--enable-extensions\")\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")\n",
    "chrome_options.add_argument(\"--enable-blink-features=AutomaticCookieManagementEnabled\")\n",
    "chrome_options.add_argument(\"--user-data-dir=C:\\\\Users\\\\mjzag\\\\AppData\\\\Local\\\\Google\\\\Chrome for Testing\\\\User Data\")\n",
    "chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "\n",
    "# Inicializar o driver do Chrome com as configurações do perfil\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# Defina o número de segundos de atraso entre cada busca\n",
    "atraso_entre_buscas = 20  # Altere conforme necessário\n",
    "\n",
    "try:\n",
    "    # Abrir a página de busca\n",
    "    driver.get(\"https://www.sicredi.com.br/site/busca-resultado/\")\n",
    "\n",
    "    # Iterar sobre as informações da coluna NOME_INSTITUICAO de cooperativas_sicredi\n",
    "    for index, row in cooperativas_sicredi.iterrows():\n",
    "        # Obter o valor da coluna \"NOME_INSTITUICAO\" atual\n",
    "        search_query = row['NOME_INSTITUICAO']\n",
    "\n",
    "        # Esperar até que o campo de busca esteja disponível\n",
    "        search_box = WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \".blocoFormulario input.inputGeral\"))\n",
    "        )\n",
    "\n",
    "        # Limpar o campo de busca\n",
    "        search_box.clear()\n",
    "\n",
    "        # Preencher o campo de busca com o valor atual da coluna\n",
    "        search_box.send_keys(search_query)\n",
    "        search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "        # Esperar até que o contêiner de resultados da busca esteja presente\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"gs-webResult.gs-result\"))\n",
    "        )\n",
    "\n",
    "        # Obter o HTML da página após a busca\n",
    "        page_source = driver.page_source\n",
    "\n",
    "        # Parse HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # Find the first search result element\n",
    "        first_search_result = soup.find(\"div\", class_=\"gs-webResult gs-result\")\n",
    "\n",
    "        # Extract the URL from the first search result element\n",
    "        if first_search_result:\n",
    "            url_element = first_search_result.find(\"a\", class_=\"gs-title\")\n",
    "            if url_element:\n",
    "                result_url = url_element.get(\"href\")\n",
    "                print(f\"URL para '{search_query}': {result_url}\")\n",
    "\n",
    "        # Adicionar um atraso antes da próxima busca\n",
    "        time.sleep(atraso_entre_buscas)\n",
    "\n",
    "finally:\n",
    "    # Fechar o navegador\n",
    "    driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
